{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "036f91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3711eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b387fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05fe46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema =  types.StructType([\n",
    "                    types.StructField('VendorID', types.LongType(), nullable=True),\n",
    "                    types.StructField('lpep_pickup_datetime', types.TimestampType(), nullable=True),\n",
    "                    types.StructField('lpep_dropoff_datetime', types.TimestampType(), nullable=True),\n",
    "                    types.StructField('store_and_fwd_flag', types.StringType(), nullable=True),\n",
    "                    types.StructField('RatecodeID', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('PULocationID', types.LongType(), nullable=True),\n",
    "                    types.StructField('DOLocationID', types.LongType(), nullable=True),\n",
    "                    types.StructField('passenger_count', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('trip_distance', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('fare_amount', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('extra', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('mta_tax', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('tip_amount', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('tolls_amount', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('ehail_fee', types.IntegerType(), nullable=True),\n",
    "                    types.StructField('improvement_surcharge', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('total_amount', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('payment_type', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('trip_type', types.DoubleType(), nullable=True),\n",
    "                    types.StructField('congestion_surcharge', types.DoubleType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7805ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_schema = types.StructType([\n",
    "                    types.StructField('VendorID', types.LongType(), True), \n",
    "                    types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "                    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "                    types.StructField('passenger_count', types.DoubleType(), True), \n",
    "                    types.StructField('trip_distance', types.DoubleType(), True), \n",
    "                    types.StructField('RatecodeID', types.DoubleType(), True), \n",
    "                    types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "                    types.StructField('PULocationID', types.LongType(), True), \n",
    "                    types.StructField('DOLocationID', types.LongType(), True), \n",
    "                    types.StructField('payment_type', types.LongType(), True), \n",
    "                    types.StructField('fare_amount', types.DoubleType(), True), \n",
    "                    types.StructField('extra', types.DoubleType(), True), \n",
    "                    types.StructField('mta_tax', types.DoubleType(), True), \n",
    "                    types.StructField('tip_amount', types.DoubleType(), True), \n",
    "                    types.StructField('tolls_amount', types.DoubleType(), True), \n",
    "                    types.StructField('improvement_surcharge', types.DoubleType(), True), \n",
    "                    types.StructField('total_amount', types.DoubleType(), True), \n",
    "                    types.StructField('congestion_surcharge', types.DoubleType(), True), \n",
    "                    types.StructField('airport_fee', types.DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ffdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/tania/git/DataEngineeringZoomCamp/week_5_batch_processing/code/data/pq/green/2020/01 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/pq/green/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m df_green \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mschema(green_schema)\u001b[38;5;241m.\u001b[39mload(input_path)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdf_green\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/tania/git/DataEngineeringZoomCamp/week_5_batch_processing/code/data/pq/green/2020/01 already exists."
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/green/{year}/{month:02d}/'\n",
    "  \n",
    "    df_green = spark.read.format('parquet').schema(green_schema).load(input_path)\n",
    "\n",
    "    df_green.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aae09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021 \n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/green/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read.format('parquet').schema(green_schema).load(input_path)\n",
    "\n",
    "    df_green.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n",
    "    \n",
    "    if month == 8:\n",
    "        yellow_df = spark.read.format(\"parquet\") \\\n",
    "                        .option(\"mergeSchema\", \"true\") \\\n",
    "                        .schema(yellow_schema) \\\n",
    "                        .load(input_path)\n",
    "        \n",
    "    else:\n",
    "        yellow_df = spark.read.parquet(input_path)\n",
    "        yellow_df = yellow_df.withColumn('airport_fee',  yellow_df[\"airport_fee\"].cast(types.DoubleType()))\n",
    "        yellow_df = yellow_df.withColumn(\"airport_fee\", format_number(yellow_df['airport_fee'], 2))   \n",
    "\n",
    "    yellow_df.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n",
    "    \n",
    "    if month == 8:\n",
    "        yellow_df = spark.read.format(\"parquet\") \\\n",
    "                        .option(\"mergeSchema\", \"true\") \\\n",
    "                        .schema(yellow_schema) \\\n",
    "                        .load(input_path)\n",
    "        \n",
    "    else:\n",
    "        yellow_df = spark.read.parquet(input_path)\n",
    "        yellow_df = yellow_df.withColumn('airport_fee',  yellow_df[\"airport_fee\"].cast(types.DoubleType()))\n",
    "        yellow_df = yellow_df.withColumn(\"airport_fee\", format_number(yellow_df['airport_fee'], 2))   \n",
    "\n",
    "    yellow_df.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00ac5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
